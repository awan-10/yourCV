<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.6.1">
<title>Ammar Ahmad Awan</title>
<style>
@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";

/* Default font. */
body {
font-family: "Open Sans","DejaVu Sans",sans-serif;
font-weight: 300;
margin-top: 3%;
margin-bottom: 10%;
}

h1, h2, h3, h4 {
font-style: normal;
color: #121D40;
font-weight: 300;
}

h1 {
margin-top: 1.2em;
margin-bottom: 0;
font-size: 3em;
text-align: center;
}

h2 {
font-size: 1.9em;
border-bottom: 1px solid silver;
margin-top: 1.2em;
margin-bottom: 0.5em;
}

h3 {
font-size:1.5em;
margin-top: 0.5em;
margin-bottom: 0;
}

h4 {
font-size:1.125em;
}

p, li {
margin-top: 0;
margin-bottom: 0;
}

ul {
margin-top: 0.4em;
margin-bottom: 0;
}

#header,#content {
margin-left: 10%;
margin-right: 10%;
margin-top: 0;
margin-bottom: 0;
}

#preamble {
text-align: center;
font-style: italic;
margin-top: 0;
margin-bottom: 0;
}

</style>
</head>
<body class="article">
<div id="header">
<h1>Ammar Ahmad Awan</h1>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><em>2015 Neil Ave. • Columbus • OH 43210 • USA</em><br>
<em>+1 614 360 8349 • <a href="mailto:ammar.ahmad.awan@gmail.com">ammar.ahmad.awan@gmail.com</a></em></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_education">Education</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>The Ohio State University (OSU), Columbus, Ohio, USA</strong></p>
</div>
<div class="paragraph">
<p>Ph.D. in Computer Science and Engineering, Aug 2014—May 2020 (Expected)</p>
</div>
<div class="paragraph">
<p>Advisor: <a href="http://cse.osu.edu/~panda/">D.K. Panda</a> • CGPA: 3.68/4.0</p>
</div>
<div class="paragraph">
<p>Thesis: Co-designing MPI Middleware and DL Frameworks for
High-Performance DNN Training on HPC Systems</p>
</div>
<hr>
<div class="paragraph">
<p><strong>Kyung Hee University (KHU), Suwon, South Korea</strong></p>
</div>
<div class="paragraph">
<p>Master of Computer Engineering, 2011—2013</p>
</div>
<div class="paragraph">
<p>Advisor: <a href="http://uclab.khu.ac.kr/index_professor.php?ckattempt=1">Sungyoung Lee</a> • CGPA: 4.22/4.3</p>
</div>
<div class="paragraph">
<p>Thesis: Efficient Support for Parallel File Access in Java HPC</p>
</div>
<hr>
<div class="paragraph">
<p><strong>National University of Sciences and Technology (NUST), Islamabad, Pakistan</strong></p>
</div>
<div class="paragraph">
<p>Bachelor of Information Technology, 2004—2008</p>
</div>
<div class="paragraph">
<p>Advisor: <a href="https://scholar.google.com.pk/citations?user=V0XEUMAAAAAJ&amp;hl=en">Aamir Shafi</a> • CGPA: 3.71/4.0</p>
</div>
<div class="paragraph">
<p>Final Project: Optimizing N-body Simulations for Multicore Compute Clusters</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_professional_experience">Professional Experience</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><strong>Network Based Computing Lab (<a href="http://nbcl.cse.ohio-state.edu" class="bare">http://nbcl.cse.ohio-state.edu</a>) at The Ohio State University, Columbus, OH</strong></p>
<div class="ulist">
<ul>
<li>
<p>Graduate Research Assistant (Aug ‘14 – present)</p>
<div class="ulist">
<ul>
<li>
<p>Investigate Collective Communication Designs and Implementations for CUDA-Aware MPI libraries like MVAPICH2-GDR.</p>
</li>
<li>
<p>Co-design Deep Learning frameworks like Caffe and MPI runtimes like MVAPICH2 to enable efficient distributed Deep Learning on modern GPU clusters.</p>
</li>
<li>
<p>Utilize existing benchmark suites like OSU Microbenchmarks (OMB), Intel MPI benchmarks (IMB) and test suites like MPICH tests, Intel tests, etc. to rigorously test and evaluate new designs on multiple HPC systems with diverse set of CPU and GPU architectures.</p>
</li>
<li>
<p>Perform regression/sanity testing on software stacks that are released periodically as new features from several research students and staff are developed and pushed to the main MVAPICH2 codebase.</p>
</li>
<li>
<p>Design new benchmarks to evaluate the capabilities of the MVAPICH2 MPI library as well as OSU-Caffe and other DL stacks like Horovod for TensorFlow and PyTorch on large-scale HPC systems.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>X-Scale Solutions (<a href="http://x-scalesolutions.com" class="bare">http://x-scalesolutions.com</a>), Columbus, OH</strong></p>
<div class="ulist">
<ul>
<li>
<p>Research Intern (May ‘19—Aug ’19)</p>
<div class="ulist">
<ul>
<li>
<p>Conduct in-depth performance characterization of TensorFlow/Horovod on Large Scale HPC Systems like Summit (#1 on Top500) and Sierra (#2 on Top500).</p>
</li>
<li>
<p>Implementation of user-friendly installers for X-Scale products including X-ScaleAI and X-ScaleHPC.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Microsoft Research, Redmond, WA</strong></p>
<div class="ulist">
<ul>
<li>
<p>Research Intern (May ’18 – Aug’18)</p>
<div class="ulist">
<ul>
<li>
<p>Assisted in design and evaluation of semantics-preserving SGD codes for CPUs in Azure cloud.</p>
</li>
<li>
<p>Developed code to evaluate Criteo’s Ad clicks prediction at scale using TensorFlow on Cloud-based systems like Google Cloud ML, Amazon SageMaker, and Azure BatchAI.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>iFaST Solutions Pvt. Ltd, Peshawar, Pakistan</strong></p>
<div class="ulist">
<ul>
<li>
<p>Vice President: Innovation (Jun ‘13 – Jun ‘14)</p>
<div class="ulist">
<ul>
<li>
<p>Developed tutorials and delivered talks on Version Control (GIT) and use of PHP frameworks (CodeIgniter) to transform internal processes to overcome software development delays faced by the company.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Ubiquitous Computing Laboratory, Kyung Hee University, South Korea</strong></p>
<div class="ulist">
<ul>
<li>
<p>Graduate Research Assistant (Aug ‘11 – Jun ‘13)</p>
<div class="ulist">
<ul>
<li>
<p>Co-founded the HPC over Cloud (HPCoC) project for the team.</p>
</li>
<li>
<p>Published two papers on Parallel I/O for Java HPC project.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Skylight Software Inc., CA and Islamabad, Pakistan</strong></p>
<div class="ulist">
<ul>
<li>
<p>Principle Software Engineer (Apr ‘11 – Jul ‘11)</p>
<div class="ulist">
<ul>
<li>
<p>Designed and implemented a state-charts based approach for developing efficient custom controls for a new document format proposed by Skylight.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>NUST-SEECS, Pakistan (Feb ‘08 – Nov ‘09) / University of Reading, UK (Feb ‘09 – Jun ‘09)</strong></p>
<div class="ulist">
<ul>
<li>
<p>Research Assistant</p>
<div class="ulist">
<ul>
<li>
<p>Analyzed and profiled performance of Gadget-2 code and proposed hybrid-parallelism to speed-up the simulations on multi-core clusters.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_awards_and_distinctions">Awards and Distinctions</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>“IEEE TCHPC Travel Award”</strong> for attending Supercomputing, SC ’19.</p>
</li>
<li>
<p><strong>“ACM Student Travel Award”</strong> for participating in ACM Student Research Competition at SC ’17.</p>
</li>
<li>
<p><strong>“NSF Student Travel Award”</strong> for attending ACM PPoPP ‘17.</p>
</li>
<li>
<p><strong>“Student Travel Award”</strong> for attending HotI ’17.</p>
</li>
<li>
<p><strong>“Best Student Poster Award”</strong> at ISC High-Performance Event (ISC ’19).</p>
</li>
<li>
<p><strong>“Best Paper Runner-up”</strong> EuroMPI 2016, Edinburgh, UK.</p>
</li>
<li>
<p><strong>“O’Donnell Fellowship”</strong> for Ph.D. in Computer Science and Engineering, OSU, USA (2014 - 2015).</p>
</li>
<li>
<p><strong>“Global IT Talents Program Scholarship”</strong> for Masters Degree in South Korea (2011 - 2013).</p>
</li>
<li>
<p><strong>“President’s Gold Medal”</strong> for highest CGPA in Bachelors Degree (NUST - 2008).</p>
</li>
<li>
<p><strong>“Rector’s Gold Medal”</strong> for Best Final Year Project (NUST - 2008).</p>
</li>
<li>
<p><strong>“Best Industry Project”</strong> award for the Final Year Project at NUST-SEECS Open House '08.</p>
</li>
<li>
<p><strong>“Merit Scholarship”</strong> for 7 out of 8 semesters at NUST. (Awarded to students with 3.5 and above GPA).</p>
</li>
<li>
<p><strong>“Third Prize”</strong> for presenting Project: Constella Platinum at All Pakistan software competition - Softcom '06.</p>
</li>
<li>
<p><strong>“Student Volunteer”</strong> for SC ‘08, USA. (Selected but couldn’t travel).</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_publications">Publications</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_select">Select</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A. A. Awan, K. V. Manian, C-H Chu, H. Subramoni, and DK Panda, “Optimized Large-Message Broadcast for Deep Learning Workloads: MPI, MPI+NCCL, or NCCL2?”, <strong>Parallel Computing</strong>, Volume 85, Jul '19, Pages 141-152, <a href="https://doi.org/10.1016/j.parco.2019.03.005" class="bare">https://doi.org/10.1016/j.parco.2019.03.005</a>.</p>
</li>
<li>
<p>A. A. Awan, C-H Chu, X. Lu, H. Subramoni, and D. K. Panda, “OC-DNN: Exploiting Advanced Unified Memory Capabilities in CUDA 9 and Volta GPUs for Out-of-Core DNN Training”, 25th IEEE International Conference on High-Performance Computing, Data, Analytics, and Data Science (<strong>HiPC</strong>) '18, Dec '18.</p>
</li>
<li>
<p>A. A. Awan, C-H Chu, X. Lu, H. Subramoni, and DK Panda, "Can Unified-Memory support on Pascal and Volta GPUs enable Out-of-Core DNN Training?", ISC High-Performance (<strong>ISC</strong>) '18, June '18. <strong>Best Student Poster Award</strong>.</p>
</li>
<li>
<p>A. A. Awan, K. Hamidouche, J. Hashmi, and D. K. Panda, "S-Caffe: Co-designing MPI Runtimes and Caffe for Scalable Deep Learning on Modern GPU Clusters”, 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (<strong>PPoPP</strong>) '17, Feb ’17.</p>
</li>
<li>
<p>A. A. Awan, K. Hamidouche, A. Venkatesh, and D. K. Panda, “Efficient Large Message Broadcast using NCCL and CUDA-Aware MPI for Deep Learning”, 23rd European MPI Users' Group Meeting (<strong>EuroMPI</strong>) ‘16, Sep '16. <strong>Best Paper Runner-Up</strong>.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_all_publications">All Publications</h3>
<div class="paragraph">
<p><em>Most updated list of publications is available from my <a href="https://scholar.google.com/citations?user=JM_IZzQAAAAJ&amp;hl=en">Google Scholar</a> page.</em></p>
</div>
<div class="sect3">
<h4 id="_journal_articles">Journal Articles</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A. A. Awan, A. Jain, C-H Chu, H. Subramoni, and DK Panda, “Communication Profiling and Characterization of Deep Learning Workloads on Clusters with High-Performance Interconnects”, IEEE Micro ’19 (to appear in).</p>
</li>
<li>
<p>A. A. Awan, K. V. Manian, C-H Chu, H. Subramoni, and DK Panda, “Optimized Large-Message Broadcast for Deep Learning Workloads: MPI, MPI+NCCL, or NCCL2?”, Parallel Computing, Volume 85, July 2019, Pages 141-152, <a href="https://doi.org/10.1016/j.parco.2019.03.005" class="bare">https://doi.org/10.1016/j.parco.2019.03.005</a>.</p>
</li>
<li>
<p>C-H Chu, X. Lu, A. A. Awan, H. Subramoni, Bracy Elton, and DK Panda, Exploiting Hardware Multicast and GPUDirect RDMA for Efficient Broadcast, IEEE Transactions on Parallel and Distributed Systems (TPDS '18), vol. 30, no. 3.</p>
</li>
<li>
<p>K. Hamidouche, A. Venkatesh, A. A. Awan, H. Subramoni, and D. K. Panda, “CUDA-Aware OpenSHMEM: Extensions and Designs for High Performance OpenSHMEM on GPU Clusters”, Parallel Computing, Volume 58, October 2016, Pages 27-36.</p>
</li>
<li>
<p>Z. Pervez, A. A. Awan, A. M. Khattak, S. Y. Lee, and Eui-Nam Huh, “Privacy-aware searching with oblivious term matching for cloud storage”, Journal of Supercomputing (2013).</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_refereed_conference_workshop_papers">Refereed Conference/Workshop Papers</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A. Jain, A. A. Awan, H. Subramoni, and DK Panda, “Scaling TensorFlow, PyTorch, and MXNet using MVAPICH2 for High-Performance Deep Learning on Frontera”, 3rd Deep Learning on Supercomputers Workshop, to be held in conjunction with SC ‘19</p>
</li>
<li>
<p>A. Jain, A. A. Awan, Q. Anthony, H. Subramoni, and DK Panda, “Performance Characterization of DNN Training using TensorFlow and PyTorch on Modern Clusters”, 21st IEEE International Conference on Cluster Computing, (Cluster ‘19).</p>
</li>
<li>
<p>A. A. Awan, A. Jain, C-H Chu, H. Subramoni, and D. K. Panda, “Communication Profiling and Characterization of Deep Learning Workloads on Clusters with High-Performance Interconnects”, 26th Symposium on High-Performance Interconnects (HotI ’19).</p>
</li>
<li>
<p>A. A. Awan, J. Bedorf, C-H Chu, H. Subramoni, and D. K. Panda, “Scalable Distributed DNN Training using TensorFlow and CUDA-Aware MPI: Characterization, Designs, and Performance Evaluation”, in Proceedings of IEEE/ACM CCGrid '19.</p>
</li>
<li>
<p>K. Vadambacheri Manian, A. A. Awan, A. Ruhela, C. Chu, and D. K. Panda, “Characterizing CUDA Unified Memory (UM)-Aware MPI Designs on Modern GPU Architectures”, 12th Workshop on General Purpose Processing Using GPU (GPGPU '19), held with ASPLOS '19.</p>
</li>
<li>
<p>A. A. Awan, C-H Chu, X. Lu, H. Subramoni, and D. K. Panda, “OC-DNN: Exploiting Advanced Unified Memory Capabilities in CUDA 9 and Volta GPUs for Out-of-Core DNN Training”, in Proceedings of IEEE HiPC '18.</p>
</li>
<li>
<p>A. A. Awan, C-H Chu, H. Subramoni, D. K. Panda, “Optimized Broadcast for Deep Learning Workloads on Dense-GPU InfiniBand Clusters: MPI or NCCL?”, in Proceedings of EuroMPI ’18.</p>
</li>
<li>
<p>A. A. Awan, H. Subramoni, D. K. Panda, An In-depth Performance Characterization of CPU- and GPU-based DNN Training on Modern Architectures, 3rd Workshop on Machine Learning in HPC Environments (MLHPC ‘17), held with SC ’17.</p>
</li>
<li>
<p>C-H Chu, X. Lu, A. A. Awan, H. Subramoni, J. Hashmi, Bracy Elton, and DK Panda, “Efficient and Scalable Multi-Source Streaming Broadcast on GPU Clusters for Deep Learning”, International Conference on Parallel Processing (ICPP), Aug ’17.</p>
</li>
<li>
<p>A. A. Awan, K. Hamidouche, J. Hashmi, and D. K. Panda, “S-Caffe: Co-designing MPI Runtimes and Caffe for Scalable Deep Learning on Modern GPU Clusters”, 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, Feb ’17.</p>
</li>
<li>
<p>K. Hamidouche, A. A. Awan, A. Venkatesh, and D. K. Panda, “CUDA M3: Designing Efficient CUDA Managed Memory-aware MPI by Exploiting GDR and IPC”, 23rd IEEE International Conference on High Performance Computing, Data, and Analytics, Dec ’16.</p>
</li>
<li>
<p>A. A. Awan, K. Hamidouche, A. Venkatesh, and D. K. Panda, “Efficient Large Message Broadcast using NCCL and CUDA-Aware MPI for Deep Learning”, 23rd European MPI Users' Group Meeting (EuroMPI ‘16). <strong>Best Paper Runner-Up</strong>.</p>
</li>
<li>
<p>C. Chu, K. Hamidouche, A. Venkatesh, A. A. Awan, and D. K. Panda, “CUDA Kernel based Collective Reduction Operations on Large-scale GPU Clusters”, 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid ‘16).</p>
</li>
<li>
<p>A. A. Awan, K. Hamidouche, A. Venkatesh, J. Perkins, H. Subramoni, and D. K. Panda, “GPU-Aware Design, Implementation, and Evaluation of Non-blocking Collective Benchmark”, EuroMPI ’15.</p>
</li>
<li>
<p>K. Hamidouche, A. Venkatesh, A. A. Awan, H. Subramoni, and D. K. Panda, “Exploiting GPUDirect RDMA in Designing High Performance OpenSHMEM for NVIDIA GPU Clusters” IEEE Cluster 2015, Sep ’15.</p>
</li>
<li>
<p>A. A. Awan, K. Hamidouche, C. Chu, and D. K. Panda, “A Case for Non-Blocking Collectives in OpenSHMEM: Design, Implementation, and Performance Evaluation using MVAPICH2-X”, OpenSHMEM 2015 for PGAS Programming in the Exascale Era, Aug ‘15.</p>
</li>
<li>
<p>H. Subramoni, A. A. Awan, K. Hamidouche, D. Pekurovsky, A. Venkatesh, S. Chakraborty, K. Tomko, and D. K. Panda, “Designing Non-Blocking Personalized Collectives with Near Perfect Overlap for RDMA-Enabled Clusters”, ISC High Performance 2015 (ISC '15), Jul ’15.</p>
</li>
<li>
<p>S. Chakraborty, H. Subramoni, J. Perkins, A. A. Awan, and D. K. Panda, “On-demand Connection Management for OpenSHMEM and OpenSHMEM+MPI” HIPS '15 (IPDPS Workshop), May ’15.</p>
</li>
<li>
<p>A. A. Awan, M. S. Ayub, A. Shafi and S. Lee, "Towards Efficient Support for Parallel I/O in Java HPC," 2012 13th International Conference on Parallel and Distributed Computing, Applications and Technologies, Beijing, Dec ’12.</p>
</li>
<li>
<p>M. B. Amin, W. A. Khan, A. A. Awan, and S. Y. Lee, “Intercloud Message Exchange Middleware”, 6th International Conference on Ubiquitous Information Management and Communication (ICUIMC '12).</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_posters">Posters</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A. A. Awan and DK Panda, "Co-designing Communication Middleware and Deep Learning Frameworks for High-Performance DNN Training on HPC Systems", (To be presented), Doctoral Showcase @ SC '19, Denver, CO, Nov '19.</p>
</li>
<li>
<p>A. A. Awan, H. Subramoni, and DK Panda, "Exploiting CUDA Unified Memory for Efficient Out-of-Core DNN Training", NVIDIA GTC '19, San Jose, April '19.</p>
</li>
<li>
<p>A. A. Awan, C-H Chu, X. Lu, H. Subramoni, and DK Panda, "Can Unified-Memory support on Pascal and Volta GPUs enable Out-of-Core DNN Training?", ISC High-Performance (ISC) '18, Germany, June, 2018. <strong>Best Student Poster Award</strong>.</p>
</li>
<li>
<p>A. A. Awan and DK Panda, "Co-designing MPI Runtimes and Deep Learning Frameworks for Scalable Distributed Training on GPU Clusters", ACM Student Research Competition (SRC) poster at SC '17, Denver, CO, Nov '17.</p>
</li>
<li>
<p>A. A. Awan, M. B. Amin, S. Hussain, A. Shafi, S. Y. Lee, “An MPI-IO Compliant Java based Parallel I/O Library (Poster)”, 13th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid '13), Delft, Netherlands, May ’13.</p>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_talks_tutorials">Talks/Tutorials</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>“High Performance Distributed Deep Learning: A Beginner’s Guide”</strong>, SC ’19 (To be presented).</p>
</li>
<li>
<p><strong>“High Performance Architectures for Distributed Deep Learning”</strong>, MICRO ’19, Oct 13, 2019.</p>
</li>
<li>
<p><strong>“HPC Meets Distributed Deep Learning”</strong>, Hot Interconnects (HotI) ’19, Aug 14, 2019.</p>
</li>
<li>
<p><strong>“High-Performance Distributed Deep Learning: A Beginner&#8217;s Guide”</strong>, PEARC '19, Jul 29, 2019.</p>
</li>
<li>
<p><strong>“High-Performance Distributed Deep Learning: A Beginner&#8217;s Guide”</strong>, ISCA '19, Jun 22, 2019.</p>
</li>
<li>
<p><strong>“High-Performance Distributed Deep Learning: A Beginner&#8217;s Guide”</strong>, ISC '19, Jun 16, 2019.</p>
</li>
<li>
<p><strong>“High-Performance Distributed Deep Learning: A Beginner&#8217;s Guide”</strong>, CCGrid '19, May 15, 2019.</p>
</li>
<li>
<p><strong>“High-Performance Distributed Deep Learning: A Beginner&#8217;s Guide”</strong>, NCAR SEA '19, Apr 12, 2019.</p>
</li>
<li>
<p><strong>“How to Boost the Performance of HPC/AI Applications Using MVAPICH2 Library”</strong> GTC '19, Mar 20, 2019.</p>
</li>
<li>
<p><strong>“High-Performance Distributed Deep Learning: A Beginner&#8217;s Guide”</strong>, GTC '19, Mar 18, 2019.</p>
</li>
<li>
<p><strong>“High-Performance Distributed Deep Learning: A Beginner&#8217;s Guide”</strong>, PPoPP '19, Feb 17, 2019.</p>
</li>
<li>
<p><strong>“High-Performance Distributed Deep Learning: A Beginner&#8217;s Guide”</strong>, DOD-PETTT '18, May 15, 2018.</p>
</li>
<li>
<p><strong>“High-Performance Distributed Deep Learning: A Beginner&#8217;s Guide”</strong>, NCAR SEA '18, Apr 5, 2018.</p>
</li>
<li>
<p><strong>“High-Performance Distributed Deep Learning: A Beginner&#8217;s Guide”</strong>, PPoPP '18, Feb 25, 2018.</p>
</li>
<li>
<p><strong>“High-Performance Distributed Deep Learning for Dummies”</strong>, IT4 Innovations (Austria), Jan 24, 2018.</p>
</li>
<li>
<p><strong>“High Performance Distributed Deep Learning for Dummies”</strong>, Hot Interconnects (HotI) '17, Aug 28, 2017.</p>
</li>
<li>
<p><strong>“S-Caffe: Co-designing MPI Runtimes and Caffe for Scalable Deep Learning on Modern GPU Clusters”</strong>, PPoPP ’17, Austin TX.</p>
</li>
<li>
<p><strong>“Efficient Large Message Broadcast using NCCL and CUDA-Aware MPI for Deep Learning”</strong>, Best Paper Runner-up Session, EuroMPI ’16 @ EPCC Edinburgh UK.</p>
</li>
<li>
<p><strong>“Why Execution is more important than Ideas”</strong>, Invited Talk at CECOS University, Peshawar, Pakistan.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_professional_service">Professional Service</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_reviewer">Reviewer</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>PyOhio ’19.</p>
</li>
<li>
<p>32nd ACM International Conference on Supercomputing (ICS ‘18).</p>
</li>
<li>
<p>Intl. Conference on High Performance Computing, Networking, Storage, and Analysis (SC ’17).</p>
</li>
<li>
<p>17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID ‘17).</p>
</li>
<li>
<p>26th International Conference on Parallel Architectures and Compilation Techniques (PACT ‘17).</p>
</li>
<li>
<p>31st IEEE International Parallel &amp; Distributed Processing Symposium (IPDPS ‘17).</p>
</li>
<li>
<p>ISC High Performance 2016 (ISC ’16).</p>
</li>
<li>
<p>Elsevier Journal of Parallel and Distributed Computing.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_volunteer">Volunteer</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>OSU Booth, Supercomputing (SC) '17, '18, and '19.</p>
</li>
<li>
<p>MVAPICH Users Group Meeting (MUG) ’16, ’17, and ’19.</p>
</li>
<li>
<p>IEEE ICDCS 2015.</p>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_technical_skills">Technical Skills</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Strong programming skills in C and Java (SE)/Java for HPC.</p>
</li>
<li>
<p>Development experience in C and interaction of C, C, and MPI (Caffe, OSU-Caffe, and OC-Caffe).</p>
</li>
<li>
<p>Basic Python programming</p>
</li>
<li>
<p>Product-development experience (Skylight Software) using C and Win32 programming.</p>
</li>
<li>
<p>Experience of developing parallel programs using OpenMP, MPI and MPJ Express.</p>
</li>
<li>
<p>Familiar with C#, ASP.NET, Android SDK, PHP, MySQL, IBM Cell SDK, and PerfAPI (PAPI)/Perfex.</p>
</li>
<li>
<p>Understanding of web technologies including HTML, DHTML, CSS, XML, XSLT and XPath.</p>
</li>
<li>
<p>Strong communication and presentation skills</p>
<div class="ulist">
<ul>
<li>
<p>Delivered several elaborate presentations on technical projects like OSU-Caffe, High-Performance Deep Learning (HiDL), MVAPICH2, Constella, Gadget-2, Oil Reservoir Simulators, and MPJ-IO.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
</body>
</html>