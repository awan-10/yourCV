<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.6.1">
<title>Ammar Ahmad Awan</title>
<style>
@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";

/* Default font. */
body {
font-family: "Open Sans","DejaVu Sans",sans-serif;
font-weight: 300;
margin-top: 3%;
margin-bottom: 10%;
}

h1, h2, h3, h4 {
font-style: normal;
color: #005582;
font-weight: 500;
}

h1 {
margin-top: 1.2em;
margin-bottom: 0;
font-size: 2.5em;
text-align: center;
}

h2 {
font-size: 1.5em;
border-bottom: 1px solid silver;
margin-top: 1.0em;
margin-bottom: 0.5em;
}

h3 {
font-size:1.3em;
margin-top: 0.5em;
margin-bottom: 0;
}

h4 {
font-size:1.125em;
}

p, li {
margin-top: 0;
margin-bottom: 0;
}

ul {
margin-top: 0.4em;
margin-bottom: 0;
}

#header,#content {
margin-left: 5%;
margin-right: 5%;
margin-top: 0;
margin-bottom: 0;
}

#preamble {
text-align: center;
font-style: italic;
margin-top: 0;
margin-bottom: 0;
}

</style>
</head>
<body class="article">
<div id="header">
<h1>Ammar Ahmad Awan</h1>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><em>2015 Neil Ave. • Columbus • OH 43210 • USA</em><br>
<em>+1 614 360 8349 • <a href="mailto:ammar.ahmad.awan@gmail.com">ammar.ahmad.awan@gmail.com</a></em><br>
<em><a href="http://cse.osu.edu/~awan.10" class="bare">http://cse.osu.edu/~awan.10</a></em></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_research_interests">Research Interests</h2>
<div class="sectionbody">
<div class="paragraph">
<p>My broad interests lie at the interesection of High
Performance Computing (HPC) and Machine Learning (ML). I am
actively investigating new approaches to improve performance
and productivity of scalable software systems for HPC and
ML.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_education">Education</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>The Ohio State University (OSU), Columbus, Ohio, USA</strong></p>
</div>
<div class="paragraph">
<p>Ph.D. in Computer Science and Engineering, Aug 2014—May 2020 (Expected)</p>
</div>
<div class="paragraph">
<p>Advisor: <a href="http://cse.osu.edu/~panda/">D.K. Panda</a> • CGPA: 3.68/4.0</p>
</div>
<div class="paragraph">
<p>Thesis: Co-designing MPI Middleware and DL Frameworks for
High-Performance DNN Training on HPC Systems</p>
</div>
<hr>
<div class="paragraph">
<p><strong>Kyung Hee University (KHU), Suwon, South Korea</strong></p>
</div>
<div class="paragraph">
<p>Master of Computer Engineering, 2011—2013</p>
</div>
<div class="paragraph">
<p>Advisor: <a href="http://uclab.khu.ac.kr/index_professor.php?ckattempt=1">Sungyoung Lee</a> • CGPA: 4.22/4.3</p>
</div>
<div class="paragraph">
<p>Thesis: Efficient Support for Parallel File Access in Java HPC</p>
</div>
<hr>
<div class="paragraph">
<p><strong>National University of Sciences and Technology (NUST), Islamabad, Pakistan</strong></p>
</div>
<div class="paragraph">
<p>Bachelor of Information Technology, 2004—2008</p>
</div>
<div class="paragraph">
<p>Advisor: <a href="https://scholar.google.com.pk/citations?user=V0XEUMAAAAAJ&amp;hl=en">Aamir Shafi</a> • CGPA: 3.71/4.0</p>
</div>
<div class="paragraph">
<p>Final Project: Optimizing N-body Simulations for Multicore Compute Clusters</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_select_publications">Select Publications</h2>
<div class="sectionbody">
<div class="paragraph">
<p><em>I am the lead author of the following publications.</em></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>A. A. Awan</strong>, A. Jain, Q. Anthony, H. Subramoni, and DK
Panda, <em>HyPar-Flow: Exploiting MPI and Keras for Scalable
Hybrid-Parallel DNN Training using TensorFlow</em>, ISC
High-Performance (<strong>ISC '20</strong>), June 2020 (Accepted to be
presented).</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, C-H Chu, X. Lu, H. Subramoni, and D. K. Panda,
<em>OC-DNN: Exploiting Advanced Unified Memory Capabilities in
CUDA 9 and Volta GPUs for Out-of-Core DNN Training</em>, 25th
IEEE International Conference on High-Performance Computing,
Data, Analytics, and Data Science (<strong>HiPC '18</strong>) '18, Dec 2018.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, C-H Chu, X. Lu, H. Subramoni, and DK Panda,
<em>Can Unified-Memory support on Pascal and Volta GPUs enable
Out-of-Core DNN Training?</em>, ISC High-Performance (<strong>ISC
'18</strong>), June 2018. <strong>Best Student Poster Award</strong>.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, K. Hamidouche, J. Hashmi, and D. K. Panda,
<em>S-Caffe: Co-designing MPI Runtimes and Caffe for Scalable
Deep Learning on Modern GPU Clusters</em>, 22nd ACM SIGPLAN
Symposium on Principles and Practice of Parallel Programming
(<strong>PPoPP '17</strong>), Feb 2017.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, K. Hamidouche, A. Venkatesh, and D. K. Panda,
<em>Efficient Large Message Broadcast using NCCL and CUDA-Aware
MPI for Deep Learning</em>, 23rd European MPI Users' Group
Meeting (<strong>EuroMPI '16</strong>), Sep 2016. <strong>Best Paper Runner-Up</strong>.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_awards_and_distinctions">Awards and Distinctions</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p><em>IEEE TCHPC Travel Award</em> for presenting Doctoral Showcase at SC ’19.</p>
</li>
<li>
<p><em>ACM Student Travel Award</em> for participating in ACM Student Research Competition at SC ’17.</p>
</li>
<li>
<p><em>NSF Student Travel Award</em> for presenting S-Caffe at ACM PPoPP ‘17.</p>
</li>
<li>
<p><em>Student Travel Award</em> for presenting Tutorial at HotI ’17.</p>
</li>
<li>
<p><em>Best Student Poster Award</em> at ISC High-Performance Event (ISC ’19).</p>
</li>
<li>
<p><em>Best Paper Runner-up</em> at EuroMPI 2016, Edinburgh, UK.</p>
</li>
<li>
<p><em>O’Donnell Fellowship</em> (5/1,400 applicants) for
first year of Ph.D. studies at The Ohio State University
(2015).</p>
</li>
<li>
<p><em>Global IT Talents Scholarship</em> for Masters Degree in South Korea (2011 - 2013).</p>
</li>
<li>
<p><em>President’s Gold Medal</em> for highest CGPA in Bachelors Degree (NUST - 2008).</p>
</li>
<li>
<p><em>Rector’s Gold Medal</em> for Best Final Year Project (NUST - 2008).</p>
</li>
<li>
<p><em>Best Industry Project Award</em> for the Final Year Project at NUST-SEECS Open House '08.</p>
</li>
<li>
<p><em>Merit Scholarship</em> for 7 out of 8 semesters at NUST. (Awarded to students with 3.5 and above GPA).</p>
</li>
<li>
<p><em>Third Prize</em> for presenting Project: Constella Platinum at All Pakistan software competition - Softcom '06.</p>
</li>
<li>
<p><em>Student Volunteer</em> for SC ‘08, USA. (Selected but couldn’t travel).</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_research_and_development_experience">Research and Development Experience</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><strong>Network Based Computing Lab (<a href="http://nbcl.cse.ohio-state.edu" class="bare">http://nbcl.cse.ohio-state.edu</a>) at The Ohio State University</strong></p>
<div class="ulist">
<ul>
<li>
<p>Graduate Research Assistant (Aug ‘14 – present)</p>
<div class="ulist">
<ul>
<li>
<p>Investigate Collective Communication Designs and
Implementations for CUDA-Aware MPI libraries like <em>MVAPICH2</em>
and <em>MVAPICH2-GDR</em>.</p>
</li>
<li>
<p>Co-design Deep Learning frameworks like Caffe and MPI
runtimes like <em>MVAPICH2</em> to enable efficient distributed Deep Learning on modern GPU clusters.</p>
</li>
<li>
<p>Utilize existing benchmark suites like OSU Microbenchmarks (OMB), Intel MPI benchmarks (IMB) and test suites like MPICH tests, Intel tests, etc. to rigorously test and evaluate new designs on multiple HPC systems with diverse set of CPU and GPU architectures.</p>
</li>
<li>
<p>Perform regression/sanity testing on software stacks
that are released periodically as new features from several
research students and staff are developed and pushed to the
main <em>MVAPICH2</em> codebase.</p>
</li>
<li>
<p>Design new benchmarks to evaluate the capabilities of the MVAPICH2 MPI library as well as OSU-Caffe and other DL stacks like Horovod for TensorFlow and PyTorch on large-scale HPC systems.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>Note: <em>MVAPICH2</em> is a popular and open-source MPI
Library being used by more than 3,000 organizations around
the world. It has been downloded 613,000 times directly from
the project site (<a href="http://mvapich.cse.ohio-state.edu" class="bare">http://mvapich.cse.ohio-state.edu</a>).</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>X-Scale Solutions (<a href="http://x-scalesolutions.com" class="bare">http://x-scalesolutions.com</a>), Columbus, OH</strong></p>
<div class="ulist">
<ul>
<li>
<p>Research Intern (May ‘19—Aug ’19)</p>
<div class="ulist">
<ul>
<li>
<p>Conducted in-depth performance characterization of
TensorFlow/Horovod on Large Scale HPC Systems like Summit
(#1 on Top500) and Sierra (#2 on Top500) using X-ScaleAI.</p>
</li>
<li>
<p>Implemented one-click installers for X-Scale products
(X-ScaleAI and X-ScaleHPC).</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Microsoft Research (MSR), Redmond, WA</strong></p>
<div class="ulist">
<ul>
<li>
<p>Research Intern with the RiSE Group at MSR (May ’18 – Aug’18)</p>
<div class="ulist">
<ul>
<li>
<p>Mentors: Madan Musuvathi, Todd Mytkowicz, and Saeed Maleki.</p>
</li>
<li>
<p>Assisted in design and evaluation of
semantics-preserving SGD codes that scale to hundreds of
CPUs.</p>
</li>
<li>
<p>Designed and developed code/experiments to evaluate
Criteo’s Ad-click prediction at scale using TensorFlow on Cloud-based systems like Google Cloud ML, Amazon SageMaker, and Azure BatchAI.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>iFaST Solutions Pvt. Ltd, Peshawar, Pakistan</strong></p>
<div class="ulist">
<ul>
<li>
<p>Vice President: Innovation (Jun ‘13 – Jun ‘14)</p>
<div class="ulist">
<ul>
<li>
<p>Developed tutorials and delivered talks on Version
Control (Git) and use of PHP frameworks (CodeIgniter) to
transform internal processes. This helped to avoid software
development delays faced by the company.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Ubiquitous Computing Laboratory, Kyung Hee University, South Korea</strong></p>
<div class="ulist">
<ul>
<li>
<p>Graduate Research Assistant (Aug ‘11 – Jun ‘13)</p>
<div class="ulist">
<ul>
<li>
<p>Co-founded the HPC over Cloud (HPCoC) project for the team.</p>
</li>
<li>
<p>Published two papers on Parallel I/O for Java HPC project.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Skylight Software Inc., CA and Islamabad, Pakistan</strong></p>
<div class="ulist">
<ul>
<li>
<p>Principle Software Engineer (Apr ‘11 – Jul ‘11)</p>
<div class="ulist">
<ul>
<li>
<p>Designed and implemented a state-charts based approach for developing efficient custom controls for a new document format proposed by Skylight.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p><strong>NUST-SEECS, Pakistan (Feb ‘08 – Nov ‘09) / University of Reading, UK (Feb ‘09 – Jun ‘09)</strong></p>
<div class="ulist">
<ul>
<li>
<p>Research Assistant</p>
<div class="ulist">
<ul>
<li>
<p>Analyzed and profiled performance of Gadget-2 code and proposed hybrid-parallelism to speed-up the simulations on multi-core clusters.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_teaching_and_mentoring_experience">Teaching and Mentoring Experience</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Mentored undergradute and graduate students at The Ohio State University to work on various research and development projects.</p>
<div class="ulist">
<ul>
<li>
<p>Arpan Jain, Ph.D. student at OSU</p>
</li>
<li>
<p>Quentin Anthony, Ph.D. Student at OSU</p>
</li>
<li>
<p>Vardaan Gangal, B.S Student at OSU</p>
</li>
</ul>
</div>
</li>
<li>
<p>Mentored seven prospective M.S and Ph.D. students for GradAppLab (<a href="http://gradapplab.pk" class="bare">http://gradapplab.pk</a>)</p>
</li>
<li>
<p>Developed and designed the overall curriculum, lectures, homework assignments, and labs for special-topic graduate course at OSU: <em>CSE 5194.01: Introduction to High Performance Deep Learning</em> (Autumn '18 and Autumn '19)</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_all_publications">All Publications</h2>
<div class="sectionbody">
<div class="paragraph">
<p><em>Most updated list of publications is available from my <a href="https://scholar.google.com/citations?user=JM_IZzQAAAAJ&amp;hl=en">Google Scholar</a> page.</em></p>
</div>
<div class="sect2">
<h3 id="_journal_articles">Journal Articles</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>A. A. Awan</strong>, A. Jain, C-H Chu, H. Subramoni, and DK Panda,
<em>Communication Profiling and Characterization of Deep
Learning Workloads on Clusters with High-Performance
Interconnects</em>, IEEE Micro (Early Access: doi:
10.1109/MM.2019.2949986).</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, K. V. Manian, C-H Chu, H. Subramoni, and DK
Panda, <em>Optimized Large-Message Broadcast for Deep Learning
Workloads: MPI, MPI+NCCL, or NCCL2?</em>, Parallel Computing
(PARCO '19), Vol. 85, Pages 141-152, July 2019.</p>
</li>
<li>
<p>C-H Chu, X. Lu, <strong>A. A. Awan</strong>, H. Subramoni, Bracy Elton, and
DK Panda, <em>Exploiting Hardware Multicast and GPUDirect RDMA
for Efficient Broadcast</em>, IEEE Transactions on Parallel and
Distributed Systems (TPDS '19), Vol. 30, No. 3, Pages
575-588, Mar 2019.</p>
</li>
<li>
<p>K. Hamidouche, A. Venkatesh, <strong>A. A. Awan</strong>, H. Subramoni, and D. K.
Panda, <em>CUDA-Aware OpenSHMEM: Extensions and Designs
for High Performance OpenSHMEM on GPU Clusters</em>, Parallel
Computing (PARCO '16), Vol. 58, Pages 27-36, Oct 2016.</p>
</li>
<li>
<p>Z. Pervez, <strong>A. A. Awan</strong>, A. M. Khattak, S. Y. Lee, and
Eui-Nam Huh, <em>Privacy-aware searching with oblivious term
matching for cloud storage</em>, Journal of Supercomputing, Vol.
63, Issue 2, Pages 538–560, Feb 2013.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_refereed_conference_workshop_papers">Refereed Conference/Workshop Papers</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>A. A. Awan</strong>, A. Jain, Q. Anthony, H. Subramoni, and DK
Panda, <em>HyPar-Flow: Exploiting MPI and Keras for Scalable
Hybrid-Parallel DNN Training using TensorFlow</em>, ISC
High-Performance (<strong>ISC '20</strong>), June 2020 (Accepted to be
presented).</p>
</li>
<li>
<p>A. Jain, <strong>A. A. Awan</strong>, H. Subramoni, and DK Panda, <em>Scaling
TensorFlow, PyTorch, and MXNet using MVAPICH2 for
High-Performance Deep Learning on Frontera</em>, 3rd Deep
Learning on Supercomputers Workshop, held in
conjunction with SC ‘19, Nov 2019.</p>
</li>
<li>
<p>A. Jain, <strong>A. A. Awan</strong>, Q. Anthony, H. Subramoni, and DK
Panda, <em>Performance Characterization of DNN Training using
TensorFlow and PyTorch on Modern Clusters</em>, 21st IEEE
International Conference on Cluster Computing, (Cluster
'19), Sep 2019.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, A. Jain, C-H Chu, H. Subramoni, and D. K.
Panda, <em>Communication Profiling and Characterization of Deep
Learning Workloads on Clusters with High-Performance
Interconnects</em>, 26th Symposium on High-Performance
Interconnects (HotI ’19), Aug 2019.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, J. Bedorf, C-H Chu, H. Subramoni, and D. K. Panda,
<em>Scalable Distributed DNN Training using TensorFlow and
CUDA-Aware MPI: Characterization, Designs, and Performance
Evaluation</em>, 19th IEEE/ACM International Symposium on
Cluster, Cloud and Grid Computing (CCGrid '19), May 2019.</p>
</li>
<li>
<p>K. Vadambacheri Manian, <strong>A. A. Awan</strong>, A. Ruhela, C. Chu,
and D. K. Panda, <em>Characterizing CUDA Unified Memory (UM)-Aware
MPI Designs on Modern GPU Architectures</em>, 12th Workshop on
General Purpose Processing Using GPU (GPGPU '19), held in
conjunction with
ASPLOS '19, Apr 2019.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, C-H Chu, X. Lu, H. Subramoni, and D. K. Panda,
<em>OC-DNN: Exploiting Advanced Unified Memory Capabilities in
CUDA 9 and Volta GPUs for Out-of-Core DNN Training</em>, IEEE
25th International Conference on High Performance Computing
(HiPC '18), Dec 2018.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, C-H Chu, H. Subramoni, D. K. Panda, <em>Optimized
Broadcast for Deep Learning Workloads on Dense-GPU
InfiniBand Clusters: MPI or NCCL?</em>, 25th European MPI Users'
Group Meeting (EuroMPI '18), Sep 2018.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, H. Subramoni, D. K. Panda, <em>An In-depth
Performance Characterization of CPU- and GPU-based DNN
Training on Modern Architectures</em>, 3rd Workshop on Machine
Learning in HPC Environments (MLHPC ‘17), held in
conjunction with SC ’17, Nov 2017.</p>
</li>
<li>
<p>C-H Chu, X. Lu, <strong>A. A. Awan</strong>, H. Subramoni, J. Hashmi, Bracy
Elton, and DK Panda, <em>Efficient and Scalable Multi-Source
Streaming Broadcast on GPU Clusters for Deep Learning</em>,
46th International Conference on Parallel Processing (ICPP '17), Aug
2017.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, K. Hamidouche, J. Hashmi, and D. K. Panda,
<em>S-Caffe: Co-designing MPI Runtimes and Caffe for Scalable
Deep Learning on Modern GPU Clusters</em>, 22nd ACM SIGPLAN
Symposium on Principles and Practice of Parallel Programming
(PPoPP '17), Feb 2017.</p>
</li>
<li>
<p>K. Hamidouche, <strong>A. A. Awan</strong>, A. Venkatesh, and D. K. Panda,
<em>CUDA M3: Designing Efficient CUDA Managed Memory-aware MPI
by Exploiting GDR and IPC</em>, 23rd IEEE International
Conference on High Performance Computing, Data, and
Analytics, Dec 2016.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, K. Hamidouche, A. Venkatesh, and D. K. Panda,
<em>Efficient Large Message Broadcast using NCCL and CUDA-Aware
MPI for Deep Learning</em>, 23rd European MPI Users' Group
Meeting (EuroMPI ‘16), Sep 2016. <strong>Best Paper Runner-Up</strong>.</p>
</li>
<li>
<p>C. Chu, K. Hamidouche, A. Venkatesh, <strong>A. A. Awan</strong>, and D. K.
Panda, <em>CUDA Kernel based Collective Reduction Operations on
Large-scale GPU Clusters</em>, 16th IEEE/ACM International
Symposium on Cluster, Cloud and Grid Computing (CCGrid
‘16), May 2016.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, K. Hamidouche, A. Venkatesh, J. Perkins, H.
Subramoni, and D. K. Panda, <em>GPU-Aware Design,
Implementation, and Evaluation of Non-blocking Collective
Benchmark</em>, 22nd European MPI Users' Group
Meeting (EuroMPI ‘15), Sep 2015.</p>
</li>
<li>
<p>K. Hamidouche, A. Venkatesh, <strong>A. A. Awan</strong>, H. Subramoni,
and D. K. Panda, <em>Exploiting GPUDirect RDMA in Designing High
Performance OpenSHMEM for NVIDIA GPU Clusters</em>, IEEE
International Conference on Cluster Computing (Cluster '15),
Sep 2015.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, K. Hamidouche, C. Chu, and D. K. Panda, <em>A
Case for Non-Blocking Collectives in OpenSHMEM: Design,
Implementation, and Performance Evaluation using
MVAPICH2-X</em>, Workshop on OpenSHMEM and Related Technologies
(OpenSHMEM '15), Aug 2015.</p>
</li>
<li>
<p>H. Subramoni, <strong>A. A. Awan</strong>, K. Hamidouche, D. Pekurovsky, A.
Venkatesh, S. Chakraborty, K. Tomko, and D. K. Panda,
<em>Designing Non-Blocking Personalized Collectives with Near
Perfect Overlap for RDMA-Enabled Clusters</em>, ISC High
Performance (ISC '15), Jul 2015.</p>
</li>
<li>
<p>S. Chakraborty, H. Subramoni, J. Perkins, <strong>A. A. Awan</strong>,
and D. K. Panda, <em>On-demand Connection Management for OpenSHMEM
and OpenSHMEM+MPI</em> (HIPS '15), IPDPS Workshop, May 2015.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, M. S. Ayub, A. Shafi and S. Lee, <em>Towards
Efficient Support for Parallel I/O in Java HPC</em>, 13th
International Conference on Parallel and Distributed
Computing, Applications and Technologies (PDCAT '12), Dec
2012.</p>
</li>
<li>
<p>M. B. Amin, W. A. Khan, <strong>A. A. Awan</strong>, and S. Y. Lee,
“Intercloud Message Exchange Middleware”, 6th International
Conference on Ubiquitous Information Management and
Communication (ICUIMC '12), Sep 2012.</p>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_posters">Posters</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>A. A. Awan</strong> and DK Panda, <em>Co-designing Communication
Middleware and Deep Learning Frameworks for High-Performance
DNN Training on HPC Systems</em>, Doctoral Showcase at SC '19,
Nov 2019.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, H. Subramoni, and DK Panda, <em>Exploiting CUDA
Unified Memory for Efficient Out-of-Core DNN Training</em>,
Poster at NVIDIA GTC '19, April 2019.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, C-H Chu, X. Lu, H. Subramoni, and DK Panda,
<em>Can Unified-Memory support on Pascal and Volta GPUs enable
Out-of-Core DNN Training?</em>, ISC High-Performance (ISC '18),
Jun 2018. <strong>Best Student Poster Award</strong>.</p>
</li>
<li>
<p><strong>A. A. Awan</strong> and DK Panda, <em>Co-designing MPI Runtimes and
Deep Learning Frameworks for Scalable Distributed Training
on GPU Clusters</em>, ACM Student Research Competition (SRC)
poster at SC '17, Nov 2017.</p>
</li>
<li>
<p><strong>A. A. Awan</strong>, M. B. Amin, S. Hussain, A. Shafi, S. Y. Lee,
<em>An MPI-IO Compliant Java based Parallel I/O Library</em>,
Poster at 13th IEEE/ACM International Symposium on Cluster,
Cloud and Grid Computing (CCGrid '13), May 2013.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_talks">Talks</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p><em>Benchmarking Deep Learning Workloads on Large-scale HPC
Systems</em> (Invited Talk), Benchmarking in the Data Center Workshop, PPoPP
'20, Feb 2020.</p>
</li>
<li>
<p><em>Co-designing Communication
Middleware and Deep Learning Frameworks for High-Performance
DNN Training on HPC Systems</em>, Doctoral Showcase Presentation
at SC '19, Nov 2019.</p>
</li>
<li>
<p><em>An In-depth
Performance Characterization of CPU- and GPU-based DNN
Training on Modern Architectures</em>, MLHPC ‘17, SC '17
Workshop, Nov 2017.</p>
</li>
<li>
<p><em>S-Caffe: Co-designing MPI Runtimes and Caffe for Scalable
Deep Learning on Modern GPU Clusters</em>, PPoPP ’17, Feb 2017.</p>
</li>
<li>
<p><em>Efficient Large Message Broadcast using NCCL and
CUDA-Aware MPI for Deep Learning</em>, Best Paper Runner-up
Session, EuroMPI ’16 @ EPCC Edinburgh UK, Sep 2016.</p>
</li>
<li>
<p><em>Why Execution is more important than Ideas</em>, Invited Talk
at CECOS University, Peshawar, Pakistan, Feb 2014.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_invited_tutorials">Invited Tutorials</h2>
<div class="sectionbody">
<div class="paragraph">
<p><em>Number of Attendees are in parentheses.</em></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><em>High Performance Distributed Deep Learning: A Beginner’s Guide</em>, NVIDIA GTC ’20 (Accepted; To be presented).</p>
</li>
<li>
<p><em>High Performance Distributed Deep Learning</em>, PPoPP '20, Feb 2020. (25)</p>
</li>
<li>
<p><em>High Performance Distributed Deep Learning: A Beginner’s
Guide</em>, SC ’19, Nov 2019. (120)</p>
</li>
<li>
<p><em>High Performance Architectures for Distributed Deep
Learning</em>, MICRO ’19, Oct 13, 2019. (60)</p>
</li>
<li>
<p><em>HPC Meets Distributed Deep Learning</em>, Hot Interconnects
(HotI '19), Aug 14, 2019. (50)</p>
</li>
<li>
<p><em>High-Performance Distributed Deep Learning: A Beginner&#8217;s
Guide</em>, PEARC '19, Jul 29, 2019. (80)</p>
</li>
<li>
<p><em>High-Performance Distributed Deep Learning: A Beginner&#8217;s
Guide</em>, ISCA '19, Jun 22, 2019. (40)</p>
</li>
<li>
<p><em>High-Performance Distributed Deep Learning: A Beginner&#8217;s
Guide</em>, ISC '19, Jun 16, 2019. (40)</p>
</li>
<li>
<p><em>High-Performance Distributed Deep Learning: A Beginner&#8217;s
Guide</em>, CCGrid '19, May 15, 2019. (40)</p>
</li>
<li>
<p><em>High-Performance Distributed Deep Learning: A Beginner&#8217;s
Guide</em>, NCAR SEA '19, Apr 12, 2019. (10)</p>
</li>
<li>
<p><em>How to Boost the Performance of HPC/AI Applications Using
MVAPICH2 Library</em> NVIDIA GTC '19, Mar 20, 2019. (50)</p>
</li>
<li>
<p><em>High-Performance Distributed Deep Learning: A Beginner&#8217;s
Guide</em>, NVIDIA GTC '19, Mar 18, 2019. (100)</p>
</li>
<li>
<p><em>High-Performance Distributed Deep Learning: A Beginner&#8217;s
Guide</em>, PPoPP '19, Feb 17, 2019. (15)</p>
</li>
<li>
<p><em>High-Performance Distributed Deep Learning: A Beginner&#8217;s
Guide</em>, DOD-PETTT '18, May 15, 2018. (25)</p>
</li>
<li>
<p><em>High-Performance Distributed Deep Learning: A Beginner&#8217;s
Guide</em>, NCAR SEA '18, Apr 5, 2018. (30)</p>
</li>
<li>
<p><em>High-Performance Distributed Deep Learning: A Beginner&#8217;s
Guide</em>, PPoPP '18, Feb 25, 2018. (20)</p>
</li>
<li>
<p><em>High-Performance Distributed Deep Learning for Dummies</em>,
IT4 Innovations (Austria), Jan 24, 2018. (35)</p>
</li>
<li>
<p><em>High Performance Distributed Deep Learning for Dummies</em>,
Hot Interconnects (HotI '17) Aug 28, 2017. (50)</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_professional_service">Professional Service</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_memberships">Memberships</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>ACM Student Member</p>
</li>
<li>
<p>IEEE Student Member</p>
</li>
<li>
<p>Message Passing Interface (MPI) Forum</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_reviewer">Reviewer</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>40th IEEE International Conference on Distributed Computing Systems (ICDCS '20).</p>
</li>
<li>
<p>Elsevier SoftwareX Journal</p>
</li>
<li>
<p>34th IEEE International Parallel &amp; Distributed Processing Symposium (IPDPS '20).</p>
</li>
<li>
<p>The FREE Python conference in Columbus (PyOhio ’19).</p>
</li>
<li>
<p>32nd ACM International Conference on Supercomputing (ICS ‘18).</p>
</li>
<li>
<p>Intl. Conference on High Performance Computing, Networking, Storage, and Analysis (SC ’17).</p>
</li>
<li>
<p>17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID ‘17).</p>
</li>
<li>
<p>26th International Conference on Parallel Architectures and Compilation Techniques (PACT ‘17).</p>
</li>
<li>
<p>31st IEEE International Parallel &amp; Distributed Processing Symposium (IPDPS ‘17).</p>
</li>
<li>
<p>IEEE Transactions on Parallel and Distributed Systems</p>
</li>
<li>
<p>ISC High Performance 2016 (ISC ’16).</p>
</li>
<li>
<p>Elsevier Journal of Parallel and Distributed Computing.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_volunteer">Volunteer</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>OSU Booth, Supercomputing (SC) '17, '18, and '19.</p>
</li>
<li>
<p>MVAPICH Users Group Meeting (MUG) ’16, ’17, and ’19.</p>
</li>
<li>
<p>IEEE ICDCS 2015.</p>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_technical_skills">Technical Skills</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Strong programming skills in C and Java (SE)/Java for HPC.</p>
</li>
<li>
<p>Development experience in C and interaction of C, C\, and MPI.</p>
</li>
<li>
<p>Product-development experience (Skylight Software) using C and Win32 programming.</p>
</li>
<li>
<p>Experience of developing parallel programs using OpenMP, MPI and MPJ Express.</p>
</li>
<li>
<p>Familiar with C#, ASP.NET, Android SDK, PHP, MySQL, IBM Cell SDK, and PerfAPI (PAPI)/Perfex.</p>
</li>
<li>
<p>Understanding of web technologies including HTML, DHTML, CSS, XML, XSLT and XPath.</p>
</li>
<li>
<p>Strong communication and presentation skills</p>
<div class="ulist">
<ul>
<li>
<p>Delivered several elaborate presentations on technical projects like OSU-Caffe, High-Performance Deep Learning (HiDL), MVAPICH2, Constella, Gadget-2, Oil Reservoir Simulators, and MPJ-IO.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_references">References</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Dhabaleswar Kumar (DK) Panda, Professor.</p>
<div class="literalblock">
<div class="content">
<pre>Dept. of Computer Science and Engineering
The Ohio State University
2015 Neil Avenue
Columbus, OH-43210, USA
Tel: (614) 292-5199
Email: panda@cse.ohio-state.edu
Website: http://web.cse.ohio-state.edu/~panda.2/
Twitter: @dhabalkpanda</pre>
</div>
</div>
</li>
<li>
<p>Gagan Agrawal, Professor.</p>
<div class="literalblock">
<div class="content">
<pre>School of Computer and Cyber Sciences
Augusta University
Augusta, GA 30912, USA
Email: gagrawal@augusta.edu</pre>
</div>
</div>
</li>
<li>
<p>Radu Teodorescu, Associate Professor.</p>
<div class="literalblock">
<div class="content">
<pre>Dept. of Computer Science and Engineering
The Ohio State University
2015 Neil Avenue
Columbus, OH-43210, USA
Email: teodores@cse.ohio-state.edu
Website: http://web.cse.ohio-state.edu/~teodorescu.1/</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
</body>
</html>